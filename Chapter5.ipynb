{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dde3cef",
   "metadata": {},
   "source": [
    "# Cross-Validation and the Bootstrap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd4324",
   "metadata": {},
   "source": [
    "In this lab, we explore the resampling techniques covered in this\n",
    "chapter. Some of the commands in this lab may take a while to run on\n",
    "your computer.\n",
    "\n",
    "We again begin by placing most of our imports at this top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1deb5cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.204499Z",
     "iopub.status.busy": "2025-04-03T19:33:23.204390Z",
     "iopub.status.idle": "2025-04-03T19:33:23.819021Z",
     "shell.execute_reply": "2025-04-03T19:33:23.818655Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                         summarize,\n",
    "                         poly)\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa08b62",
   "metadata": {},
   "source": [
    "There are several new imports needed for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268c41b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.820788Z",
     "iopub.status.busy": "2025-04-03T19:33:23.820641Z",
     "iopub.status.idle": "2025-04-03T19:33:23.822663Z",
     "shell.execute_reply": "2025-04-03T19:33:23.822416Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "     (cross_validate,\n",
    "      KFold,\n",
    "      ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04f8e4",
   "metadata": {},
   "source": [
    "We use the validation set approach to estimate test error rates for different linear models on the Auto dataset. The data (392 observations) are split into equal training and validation sets of 196 each using train_test_split() with test_size=196. Setting a random seed (random_state=0) ensures that the split—and thus the results—can be reproduced exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f44ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.823864Z",
     "iopub.status.busy": "2025-04-03T19:33:23.823764Z",
     "iopub.status.idle": "2025-04-03T19:33:23.829383Z",
     "shell.execute_reply": "2025-04-03T19:33:23.829160Z"
    }
   },
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                         test_size=196,\n",
    "                                         random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fe69f",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c32e917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.830793Z",
     "iopub.status.busy": "2025-04-03T19:33:23.830713Z",
     "iopub.status.idle": "2025-04-03T19:33:23.834993Z",
     "shell.execute_reply": "2025-04-03T19:33:23.834789Z"
    }
   },
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e883b8f",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of `results` evaluated on the model matrix for this model\n",
    "created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ce4f85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.836150Z",
     "iopub.status.busy": "2025-04-03T19:33:23.836082Z",
     "iopub.status.idle": "2025-04-03T19:33:23.839814Z",
     "shell.execute_reply": "2025-04-03T19:33:23.839608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(23.61661706966988)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecdee6",
   "metadata": {},
   "source": [
    "Hence our estimate for the validation MSE of  the linear regression\n",
    "fit is $23.62$.\n",
    "\n",
    "We can also estimate the validation error for\n",
    "higher-degree polynomial regressions. We first provide a function `evalMSE()` that takes a model string as well\n",
    "as training and test sets and returns the MSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a66a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.841088Z",
     "iopub.status.busy": "2025-04-03T19:33:23.841015Z",
     "iopub.status.idle": "2025-04-03T19:33:23.843031Z",
     "shell.execute_reply": "2025-04-03T19:33:23.842838Z"
    }
   },
   "outputs": [],
   "source": [
    "def evalMSE(terms,\n",
    "            response,\n",
    "            train,\n",
    "            test):\n",
    "\n",
    "   mm = MS(terms)\n",
    "   X_train = mm.fit_transform(train)\n",
    "   y_train = train[response]\n",
    "\n",
    "   X_test = mm.transform(test)\n",
    "   y_test = test[response]\n",
    "\n",
    "   results = sm.OLS(y_train, X_train).fit()\n",
    "   test_pred = results.predict(X_test)\n",
    "\n",
    "   return np.mean((y_test - test_pred)**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255779c",
   "metadata": {},
   "source": [
    "Let’s use this function to estimate the validation MSE\n",
    "using linear, quadratic and cubic fits. We use the `enumerate()`  function\n",
    "here, which gives both the values and indices of objects as one iterates\n",
    "over a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49b6999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.844166Z",
     "iopub.status.busy": "2025-04-03T19:33:23.844091Z",
     "iopub.status.idle": "2025-04-03T19:33:23.855875Z",
     "shell.execute_reply": "2025-04-03T19:33:23.855640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                       'mpg',\n",
    "                       Auto_train,\n",
    "                       Auto_valid)\n",
    "MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b8fc1",
   "metadata": {},
   "source": [
    "These error rates are $23.62, 18.76$, and $18.80$, respectively. If we\n",
    "choose a different training/validation split instead, then we\n",
    "can expect somewhat different errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac8bd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.857092Z",
     "iopub.status.busy": "2025-04-03T19:33:23.857011Z",
     "iopub.status.idle": "2025-04-03T19:33:23.868361Z",
     "shell.execute_reply": "2025-04-03T19:33:23.868136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                          test_size=196,\n",
    "                                          random_state=3)\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)],\n",
    "                       'mpg',\n",
    "                       Auto_train,\n",
    "                       Auto_valid)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2c12d",
   "metadata": {},
   "source": [
    "Using the training/validation split, the validation set errors for models with linear, quadratic, and cubic terms in horsepower are 20.76, 16.95, and 16.97, respectively. These results confirm that a quadratic model predicts mpg better than a linear model, while adding a cubic term provides no meaningful improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22daa51",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "In theory, cross-validation can be applied to any generalized linear model (GLM). In practice, however, Python’s simplest tools for cross-validation are in sklearn, which uses a different API than statsmodels. To bridge this gap, the ISLP package provides a wrapper, sklearn_sm(), that allows models fit with statsmodels to be used with sklearn’s cross-validation functions.\n",
    "\n",
    "The sklearn_sm() class takes a statsmodels model as its first argument, with optional model_str for specifying a formula and model_args for additional fitting parameters (e.g., model_args={'family': sm.families.Binomial()} for logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "601ae443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:23.869578Z",
     "iopub.status.busy": "2025-04-03T19:33:23.869501Z",
     "iopub.status.idle": "2025-04-03T19:33:24.584127Z",
     "shell.execute_reply": "2025-04-03T19:33:24.583898Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(24.231513517929212)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS,\n",
    "                      MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebadc35f",
   "metadata": {},
   "source": [
    "The cross_validate() function takes a model with fit(), predict(), and score() methods, along with feature matrix X and response Y. By specifying cv as an integer, we perform $k$-fold cross-validation; using the total number of observations gives leave-one-out CV (LOOCV). Here, the function returns several outputs, and the cross-validated test MSE is estimated to be 24.23."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f47b99",
   "metadata": {},
   "source": [
    "We can extend this approach to polynomial regressions of higher degrees. Using a for loop, we fit polynomials of degree 1 to 5, compute their cross-validation errors, and store the results in the cv_error vector, where d represents the polynomial degree. The vector is initialized beforehand, and running the loop may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11226c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:24.585528Z",
     "iopub.status.busy": "2025-04-03T19:33:24.585450Z",
     "iopub.status.idle": "2025-04-03T19:33:25.124201Z",
     "shell.execute_reply": "2025-04-03T19:33:25.123913Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03320428])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a920ae",
   "metadata": {},
   "source": [
    "Figure 5.4 shows a steep drop in test MSE from the linear to quadratic fit, with little improvement for higher-degree polynomials.\n",
    "\n",
    "The outer() method of np.power() (and similar functions like add() or min()) takes two arrays and applies the operation to every pair of elements, producing a larger array.\n",
    "\n",
    "If you want, I can make it even snappier in one sentence. Do you want me to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b64d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.125598Z",
     "iopub.status.busy": "2025-04-03T19:33:25.125513Z",
     "iopub.status.idle": "2025-04-03T19:33:25.127886Z",
     "shell.execute_reply": "2025-04-03T19:33:25.127667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [11, 13]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "np.add.outer(A, B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71385c1b",
   "metadata": {},
   "source": [
    "In the CV example above, we used $k=n$, but of course we can also use $k<n$. The code is very similar\n",
    "to the above (and is significantly faster). Here we use `KFold()` to partition the data into $k=10$ random groups. We use `random_state` to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the\n",
    "polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0f972f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.129127Z",
     "iopub.status.busy": "2025-04-03T19:33:25.129050Z",
     "iopub.status.idle": "2025-04-03T19:33:25.149889Z",
     "shell.execute_reply": "2025-04-03T19:33:25.149644Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848404, 19.13722016])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold(n_splits=10,\n",
    "           shuffle=True,\n",
    "           random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                          X,\n",
    "                          Y,\n",
    "                          cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b234093",
   "metadata": {},
   "source": [
    "The computation is much faster than LOOCV. While LOOCV could theoretically be quicker for linear models using formula (5.2), cross_validate() doesn’t utilize it. Again, cubic or higher-degree polynomials offer little improvement over a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4487a4",
   "metadata": {},
   "source": [
    "The `cross_validate()` function is flexible and can take\n",
    "different splitting mechanisms as an argument. For instance, one can use the `ShuffleSplit()`\n",
    "function to implement\n",
    "the validation set approach just as easily as $k$-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "080cdb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.151147Z",
     "iopub.status.busy": "2025-04-03T19:33:25.151072Z",
     "iopub.status.idle": "2025-04-03T19:33:25.156751Z",
     "shell.execute_reply": "2025-04-03T19:33:25.156501Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=validation);\n",
    "results['test_score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4b4cf",
   "metadata": {},
   "source": [
    "One can estimate the variability in the test error by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c46de2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.158025Z",
     "iopub.status.busy": "2025-04-03T19:33:25.157939Z",
     "iopub.status.idle": "2025-04-03T19:33:25.182100Z",
     "shell.execute_reply": "2025-04-03T19:33:25.181888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(23.802232661034164), np.float64(1.4218450941091847))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,\n",
    "                          test_size=196,\n",
    "                          random_state=0)\n",
    "results = cross_validate(hp_model,\n",
    "                         Auto.drop(['mpg'], axis=1),\n",
    "                         Auto['mpg'],\n",
    "                         cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07165f0e",
   "metadata": {},
   "source": [
    "The standard deviation shown doesn’t reflect the true sampling variability of the mean or individual test scores, since overlapping training samples create correlations, but it does indicate the Monte Carlo variation from different random folds.\n",
    "\n",
    "## The Bootstrap\n",
    "\n",
    "The bootstrap can be applied in nearly any situation without complex math. We illustrate it using the Section 5.2 example and the Auto dataset to estimate linear regression accuracy.\n",
    "\n",
    "To estimate a statistic’s accuracy, we can implement a simple bootstrap function in Python. For example, using the Portfolio dataset from ISLP, we estimate the variance of parameter $\\alpha$ (formula 5.7) with a function alpha_func() that takes a dataframe D and an index vector idx, returning the estimate of $\\alpha$ for the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b6d9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.183403Z",
     "iopub.status.busy": "2025-04-03T19:33:25.183320Z",
     "iopub.status.idle": "2025-04-03T19:33:25.186088Z",
     "shell.execute_reply": "2025-04-03T19:33:25.185890Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    "   cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    "   return ((cov_[1,1] - cov_[0,1]) /\n",
    "           (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50058e",
   "metadata": {},
   "source": [
    "This function returns an estimate for $\\alpha$\n",
    "based on applying the minimum\n",
    "    variance formula (5.7) to the observations indexed by\n",
    "the argument `idx`.  For instance, the following command\n",
    "estimates $\\alpha$ using all 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81498a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.187384Z",
     "iopub.status.busy": "2025-04-03T19:33:25.187299Z",
     "iopub.status.idle": "2025-04-03T19:33:25.189610Z",
     "shell.execute_reply": "2025-04-03T19:33:25.189413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.57583207459283)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d0aab",
   "metadata": {},
   "source": [
    "Next we randomly select\n",
    "100 observations from `range(100)`, with replacement. This is equivalent\n",
    "to constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$\n",
    "based on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64fe1cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.190738Z",
     "iopub.status.busy": "2025-04-03T19:33:25.190670Z",
     "iopub.status.idle": "2025-04-03T19:33:25.193250Z",
     "shell.execute_reply": "2025-04-03T19:33:25.193037Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6074452469619004)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio,\n",
    "           rng.choice(100,\n",
    "                      100,\n",
    "                      replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a635fe",
   "metadata": {},
   "source": [
    "This process can be generalized to create a simple function `boot_SE()` for\n",
    "computing the bootstrap standard error for arbitrary\n",
    "functions that take only a data frame as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd16bbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.194520Z",
     "iopub.status.busy": "2025-04-03T19:33:25.194445Z",
     "iopub.status.idle": "2025-04-03T19:33:25.196804Z",
     "shell.execute_reply": "2025-04-03T19:33:25.196574Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def boot_SE(func,\n",
    "            D,\n",
    "            n=None,\n",
    "            B=1000,\n",
    "            seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "                         n,\n",
    "                         replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e17ed",
   "metadata": {},
   "source": [
    "Notice the use of `_` as a loop variable in `for _ in range(B)`. This is often used if the value of the counter is\n",
    "unimportant and simply makes sure  the loop is executed `B` times.\n",
    "\n",
    "Let’s use our function to evaluate the accuracy of our\n",
    "estimate of $\\alpha$ using $B=1{,}000$ bootstrap replications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b42b4585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.198086Z",
     "iopub.status.busy": "2025-04-03T19:33:25.198011Z",
     "iopub.status.idle": "2025-04-03T19:33:25.394494Z",
     "shell.execute_reply": "2025-04-03T19:33:25.394181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09118176521277699)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func,\n",
    "                   Portfolio,\n",
    "                   B=1000,\n",
    "                   seed=0)\n",
    "alpha_SE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5464d7",
   "metadata": {},
   "source": [
    "The bootstrap estimate of ${\\rm SE}(\\hat{\\alpha})$ is 0.0912.\n",
    "\n",
    "Estimating the Accuracy of a Linear Regression Model\n",
    "\n",
    "The bootstrap can assess the variability of regression coefficients and predictions. For the Auto dataset, we use it to evaluate the intercept $\\beta_0$ and slope $\\beta_1$ of the model predicting mpg from horsepower, comparing the results to the standard SE formulas from Section 3.1.2.\n",
    "\n",
    "To apply boot_SE(), we write a function taking a dataframe D and indices idx. For bootstrapping a specific regression model, we create a generic boot_OLS() function that uses a formula. The clone() function copies the formula so it can be refit on resampled data, ensuring features like poly() are correctly recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bc11784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.396011Z",
     "iopub.status.busy": "2025-04-03T19:33:25.395917Z",
     "iopub.status.idle": "2025-04-03T19:33:25.397893Z",
     "shell.execute_reply": "2025-04-03T19:33:25.397676Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ea3ce",
   "metadata": {},
   "source": [
    "This is not quite what is needed as the first argument to\n",
    "`boot_SE()`. The first two arguments which specify the model will not change in the\n",
    "bootstrap process, and we would like to *freeze* them.   The\n",
    "function `partial()` from the `functools` module  does precisely this: it takes a function\n",
    "as an argument, and freezes some of its arguments, starting from the\n",
    "left. We use it to freeze the first two model-formula arguments of `boot_OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "740cd50c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.399076Z",
     "iopub.status.busy": "2025-04-03T19:33:25.398999Z",
     "iopub.status.idle": "2025-04-03T19:33:25.400599Z",
     "shell.execute_reply": "2025-04-03T19:33:25.400346Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d19e2",
   "metadata": {},
   "source": [
    "Typing `hp_func?` will show that it has two arguments `D`\n",
    "and `idx` --- it is a version of `boot_OLS()` with the first\n",
    "two arguments frozen --- and hence is ideal as the first argument for `boot_SE()`.\n",
    "\n",
    "The `hp_func()` function can now be used in order to create\n",
    "bootstrap estimates for the intercept and slope terms by randomly\n",
    "sampling from among the observations with replacement. We first\n",
    "demonstrate its utility on 10 bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb3ec50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.401753Z",
     "iopub.status.busy": "2025-04-03T19:33:25.401682Z",
     "iopub.status.idle": "2025-04-03T19:33:25.422622Z",
     "shell.execute_reply": "2025-04-03T19:33:25.422385Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.12226577, -0.1555926 ],\n",
       "       [37.18648613, -0.13915813],\n",
       "       [37.46989244, -0.14112749],\n",
       "       [38.56723252, -0.14830116],\n",
       "       [38.95495707, -0.15315141],\n",
       "       [39.12563927, -0.15261044],\n",
       "       [38.45763251, -0.14767251],\n",
       "       [38.43372587, -0.15019447],\n",
       "       [37.87581142, -0.1409544 ],\n",
       "       [37.95949036, -0.1451333 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "np.array([hp_func(Auto,\n",
    "          rng.choice(Auto.index,\n",
    "                     392,\n",
    "                     replace=True)) for _ in range(10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d09d96",
   "metadata": {},
   "source": [
    "Next, we use the `boot_SE()` {}  function to compute the standard\n",
    "errors of 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d561f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:25.423860Z",
     "iopub.status.busy": "2025-04-03T19:33:25.423786Z",
     "iopub.status.idle": "2025-04-03T19:33:27.089093Z",
     "shell.execute_reply": "2025-04-03T19:33:27.088869Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.731176\n",
       "horsepower    0.006092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,\n",
    "                Auto,\n",
    "                B=1000,\n",
    "                seed=10)\n",
    "hp_se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834f240",
   "metadata": {},
   "source": [
    "This indicates that the bootstrap estimate for ${\\rm SE}(\\hat{\\beta}_0)$ is\n",
    "0.85, and that the bootstrap\n",
    "estimate for ${\\rm SE}(\\hat{\\beta}_1)$ is\n",
    "0.0074.  As discussed in\n",
    "Section 3.1.2, standard formulas can be used to compute\n",
    "the standard errors for the regression coefficients in a linear\n",
    "model. These can be obtained using the `summarize()`  function\n",
    "from `ISLP.sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3888aa0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:27.090386Z",
     "iopub.status.busy": "2025-04-03T19:33:27.090313Z",
     "iopub.status.idle": "2025-04-03T19:33:27.106785Z",
     "shell.execute_reply": "2025-04-03T19:33:27.106554Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc0575",
   "metadata": {},
   "source": [
    "The standard errors for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ from the formulas in Section 3.1.2 are 0.717 and 0.006, respectively, which differ from the bootstrap estimates. This doesn’t indicate a problem with the bootstrap; rather, it reflects that the standard formulas rely on assumptions—like estimating $\\sigma^2$ from the residuals and treating $x_i$ as fixed—that may not hold. Because the data show a non-linear pattern, the linear model inflates residuals and $\\hat{\\sigma}^2$, while the bootstrap avoids these assumptions, likely giving more accurate SE estimates.\n",
    "\n",
    "Fitting a quadratic model improves the fit, and the bootstrap and standard SE estimates now align more closely for $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and $\\hat{\\beta}_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acc3e32c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:27.108124Z",
     "iopub.status.busy": "2025-04-03T19:33:27.108030Z",
     "iopub.status.idle": "2025-04-03T19:33:29.434320Z",
     "shell.execute_reply": "2025-04-03T19:33:29.434045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.538641\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.024696\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,\n",
    "                    quad_model,\n",
    "                    'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a2fd2b",
   "metadata": {},
   "source": [
    "We  compare the results to the standard errors computed using `sm.OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dca5340c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T19:33:29.435689Z",
     "iopub.status.busy": "2025-04-03T19:33:29.435602Z",
     "iopub.status.idle": "2025-04-03T19:33:29.444726Z",
     "shell.execute_reply": "2025-04-03T19:33:29.444464Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "           quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,Rmd",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
